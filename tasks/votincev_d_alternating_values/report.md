# Отчет по реализации алгоритма нахождения числа чередований знаков значений соседних элементов вектора

**Дисциплина:** Параллельное программирование  
**Преподаватель:** Нестеров Александр Юрьевич и Оболенский Арсений Андреевич  
**Студент:** Вотинцев Дмитрий Сергеевич 3823Б1ФИ3
**Вариант:** 5

## Введение
В рамках данной работы был реализован алгоритм поиска чередований знаков значений соседних элементов вектора: последовательная и параллельная реализация.

## Постановка задачи
Дан вектор v из N элементов. Необходимо найти число чередований знаков между соседними элементами.
Рассмотрим элементы v[i-1] и v[i] , i = 1,2,...,N.
Если v[i-1] >= 0  и  v[i] < 0 - то это чередование
Если v[i-1] < 0  и  v[i] >= 0 - то это чередование
(0 считается беззнаковым, грубо говоря +0)

## Описание алгоритма

Последовательная версия:
проходимся по всему массиву, сравниваем соседние элементы на чередование, увеличиваем счетчик чередований (если между соседями знак чередуется). 


## Описание схемы параллельного алгоритма
Пусть у нас N процессов, вектор размера M, тогда алгоритм такой:
0-й процесс делит массив на N частей. 
Части выделяются следующим образом:
Сначала считаем минимум, который будет каждый процесс обрабатывать:
base = M/N  
Затем вычисляем остаток от деления:
remain = M % N
Первые remain процессов заберут по 1 элементу из массива, остальные отработают только минимум.
Однако так работать не будет, вот пример:
Пусть 2 процесса и вектор {0, -1, 2, -3}
Тогда:
0й процесс возьмет 0 -1  -> всего 1 чередование
1й процесс возьмет 2 -3  -> всего 1 чередование
И ответ будет 2 - что неверно, ведь чередований 3.
Поэтому нужно ещё цеплять правого соседа:
0й возьмет 0 -1 2
1й возьмёт 2 -3 ?
Да, если просто брать правого соседа - то последний процесс всегда будет заходит за память, поэтому последний процесс будет брать на 1 элемент меньше.

Итого кратко:
вычисляем минимальную часть M/N;
первые remain процессов получают +1 элемент вектора к обработке;
всем процессам добавляем +1 чтобы цепляли правого соседа;
последнему процессу делаем -1 чтобы он не выходил за память.

Но что подразумевается под "0й процесс делит на части вектор и распределяет" ?
Можно сделать вот как: 
дается исходный вектор v
0й процесс делит его на части (прям поэлементно);
0й процесс распряделяет ЧАСТИ вектора другим процессам;
другие процессы создают внутри себя вектора, которые являются частью исходного.

Это и была моя первая реализация алгоритма - простая, но давала ускорение в 0.5.
То есть SEQ работал быстрее MPI всегда.
Проблема была не в делении на части (ведь она оптимальная) - а в памяти.
Проблема передачи памяти заключалась в том, что 0й процесс посылал части вектора - по сути копирование.
Но можно посылать индекс начала + кол-во элементов для обработки - что намного лучше и действительно дает ускорение.


## Результаты экспериментов и выводы
В таблице представлено: n - размер вектора, время выполнения SEQ и MPI версии для 4-х процессов (в секундах):

| Размер данных (n) | SEQ версия (с) | MPI версия (с) | Ускорение |
|-------------------|----------------|----------------|-----------|
| 100               | 0.000000       | 0.0013         | 0.00×     |
| 10 000            | 0.000006       | 0.0018         | 0.003×    |
| 1 000 000         | 0.0039         | 0.0059         | 0.66×     |
| 10 000 000        | 0.821          | 0.457          | 1,862×    |

При замерах (Perf тестах) была получена данная таблица.
Можно заметить, что на 4 процессах ускорение не 4, как хотелось бы,а всего 1,862x.
С увеличением размера данных будет повышаться эффективность MPI версии, но идеальное ускорение 4x недостижимо. Для достижения ускорения 4х процессы должны посылать Recv, Send так, чтобы время ожидания было минимальным, сама посылка данных должна происходить мгновенно и т.д. - но это невозможно.

 
## Заключение
В результате проделанной работы были реализованы версии MPI, SEQ алгоритма нахождения числа чередований знаков значений соседних элементов вектора.
Было так же показано, что MPI версия работает быстрее SEQ на больших значениях, а на маленьких значениях - лучше использовать SEQ.


## Литература
1. Лекции Сысоева Александра Владимировича
2. Практические занятия Нестерова Александра Юрьевича и Оболенского Арсения Андреевича
3. Интернет

## Приложения (код параллельной реализации)

```
bool VotincevDAlternatingValuesMPI::RunImpl() {
  int all_swaps = 0;

  int process_n = 0;
  MPI_Comm_size(MPI_COMM_WORLD, &process_n);  // получаю кол-во процессов

  int proc_rank = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);  // получаю ранк процесса

  // если процессов больше, чем размер вектора
  const int vector_size = static_cast<int>(vect_data_.size());
  process_n = std::min(vector_size, process_n);

  if (proc_rank == 0) {
    all_swaps = ProcessMaster(process_n);  // главный процесс (распределяет + считает часть)
  } else {
    ProcessWorker();  // процессы-рабочие (только считают)
  }

  SyncResults(all_swaps);  // посылаю результат всем процессам
  return true;
}

```